{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Document Similarity with Latent Semantic Analysis (LSA)\n",
    "\n",
    "The following notebook walks you through doing LSA document similarity in Python. We then output the document similarity matrix as a .csv file which can be manipulated to highlight similarity between documents. You then have the option of using our \"doc_sim_lsa_heatmap\" notebook to create a heatmap of cosine similarity scores between documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Before we begin\n",
    "This notebook is setup to be used specifically on a HTRC Data Capsule and has default file paths and variable settings that you may need to change. Be sure to read all the anotations and directions thoroughly so you know what changes you may wish to make and where."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include necessary packages for notebook \n",
    "\n",
    "Python's extensibility comes in large part from packages. Packages are groups of functions, data, and algorithms that allow users to easily carry out processes without recreating the wheel. Some packages are included in the basic installation of Python, others, created by Python users, are available for download.\n",
    "\n",
    "If you decide to add to the code in this notebook you may need to install packages that are not pre-installed on the Data Capsule. In your terminal, packages can be installed by simply typing `pip install nameofpackage`. However, since you are using this notebook on the HTRC Data Capsule you will not need to install any of the packages below to use this notebook as it is. We do, however, need to import the packages we want to use. Installing a package just means we have it available to use, importing the package tells Python that our code below actually utilizes the package. Below is a brief description of the packages we are using in this notebook:  \n",
    "\n",
    "- **os:** Provides a portable way of using operating system dependent functionality.\n",
    "- **sklearn:** Simple and efficient tools for data mining and data analysis built on NumPy, SciPy, and matplotlib.\n",
    "- **scipy:** Open-source software for mathematics, science, and engineering.\n",
    "- **pandas:** An open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "- **warnings:** Allows for the manipulation of warning messages in Python.\n",
    "- **numpy:** a general-purpose array-processing package designed to efficiently manipulate large multi-dimensional arrays of arbitrary records without sacrificing too much speed for small multi-dimensional arrays. \n",
    "- **string:** Contains a number of functions to process standard Python strings.\n",
    "- **nltk:** A leading platform for building Python programs to work with human language data.\n",
    "- **spacy:** A library for advanced Natural Language Processing in Python and Cython.\n",
    "\n",
    "Notice we import some of the packages differently. In some cases we just import the entire package when we say `import XYZ`. For some packages which are small, or, from which we are going to use a lot of the functionality it provides, this is fine. \n",
    "\n",
    "Sometimes when we import the package directly we say `import XYZ as X`. All this does is allow us to type `X` instead of `XYZ` when we use certain functions from the package. So we can now say `X.function()` instead of `XYZ.function()`. This saves time typing and eliminates errors from having to type out longer package names. I could just as easily type `import XYZ as potato` and whenever I use a function from the `XYZ` package I would need to type `potato.function()`. What we import the package as is up to you, but some commonly used packages have abbreviations that are standard amongst Python users such as `import pandas as pd` or `import matplotlib.pyplot as plt`. You do not need to us `pd` or `plt`, however, these are widely used and using something else could confuse other users and is generally considered bad practice. \n",
    "\n",
    "Other times we import only specific elements or functions from a package. This is common with packages that are very large and provide a lot of functionality, but from which we are only using a couple functions or a specific subset of the package that contains the functionality we need. This is seen when we say `from XYZ import ABC`. This is saying I only want the `ABC` function from the `XYZ` package. Sometimes we need to point to the specific location where a function is located within the package. We do this by adding periods in between the directory names, so it would look like `from XYZ.123.A1B2 import LMN`. This says we want the `LMN` function which is located in the `XYZ` package and then the `123` and `A1B2` directory in that package. \n",
    "\n",
    "You can also import more than one function from a package by separating the functions with commas like this `from XYZ import ABC, LMN, QRS`. This imports the `ABC`, `LMN` and `QRS` functions from the `XYZ` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will ignore deprecation and future warnings. All the warnings in this code are not concerning and will not break the code or cause errors in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings from pandas library\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning,\n",
    "                        module=\"pandas\", lineno=570)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
    "                        module = \"sklearn\", lineno = 1059)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning,\n",
    "                        module = \"sklearn\", lineno = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File paths\n",
    "Here we are saving as variables different file paths that we need in our code. We do this so that they are easier to call later and so that you can make most of your changes now and not need to make as many changes later. \n",
    "\n",
    "First we point to the `secure_volume` directory and assign that file path to the variable `home_path` since all of the volumes used will be in the `secure_volume` directory.\n",
    "\n",
    "Next, we combine the `home_path` variable with the folder names that lead to where our data is stored. Note that we do not use any file names yet, just the path to the folder. This is because we are comparing documents to one another, so we need to read in an entire directory or the contents of several directories. You will want to change the folder name(s) to match the folder names in your file path. Currently it is set to a directory of sample HathiTrust volumes.\n",
    "\n",
    "Now we add the `home_path` variable to other folder names that lead to a folder where we will want to save our document similarity matrix. You again will want to change the folder names in the path to match your own folder names. We assign this file path to the variable `cleaned_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = os.path.join(\"/media\", \"secure_volume\")\n",
    "data_home = os.path.join(home_path, \"workset\")\n",
    "cleaned_data = os.path.join(home_path, \"jupyter_notebooks\", \"document_similarity\", \"cleaned_data\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set needed variables\n",
    "\n",
    "Next ww assign values to variables that will inform various parts of our code. Just like the file path variables, this is done so you have to make fewer changes later and also to make the changes easier to find by putting them in one place.\n",
    "\n",
    "- **nltk_stop:** If you want to use the stopword list that comes with the nltk package then set `nltk_stop` equal to `True`. If you do not wish to use the nltk stopword list then set `nltk_stop` equal to `False`.\n",
    "\n",
    "- **custom_stop:** If you have created your own custom stopword list and wish to use that, then set `custom_stop` equal to `True`. If you do not have your own custom stopword list then set `custom_stop` equal to `False`.\n",
    "\n",
    "**NOTE: You can use both the nltk and custom stopword lists or you can use neither or just one or the other. You do NOT need to set them both to True or both to False. Use whatever works best for you.**\n",
    "\n",
    "- **lem:** Next we decide if we want to lemmatize our words. Lemmatizing words will turn certain words to the root of the word. So \"are\" and \"is\" become \"be\" and \"runs\" and \"running\" become \"run\". This will probably increase the similarity of documents as they will then share more words in common. If you want to lemmatize the words in your dataset then assign `True` to the variable `lem`. If you do not wish to lemmatize your words then assign `False` to the variable `lem`.\n",
    "\n",
    "- **lower_case:** Then we decide if we want all the words in our dataset lowercased. This will change \"Love\" to \"love\" so that it is recognized as the same word for similarity purposes. However, there are some cases where the use of capitalization may be important to determining similarity, so we have the option to lowercase or not. If you want to lowercase all the words in your dataset assign `True` to the variable `lower_case`. If you do not wish to lowercase all the words in your dataset then assign `False` to the variable `lower_case`.\n",
    "\n",
    "- **remove_digits:** Now we decide if we want to remove numbers from our text. Again, removing numbers will increase the similarity of texts as page numbers and other integers that may not be exactly alike will be removed. However, there are instances where numbers are thematically important, and they need to be kept. Here is where you make that decison. If you wish to remove all numbers then assign `True` to the `remove_digits` variable. If you wish to retain all numbers then assign `False` to the `remove_digits` variable.\n",
    "\n",
    "- **language:** Now we choose the language we will be using for the nltk stopwords list. If you need a different language, simply change 'english' (keep the quotes) in the `language` variable to the anglicized name of the language you wish to use (e.g. 'spanish' instead of 'espanol' or 'german' instead of 'deutsch'). For a list of available stopword languages in nltk add a new code cell and type `print(stopwords.fileids())` and the list of available languages will print out below the cell.\n",
    "\n",
    "- **lem_lang:** Now we choose the language for our lemmatizer. The languages available for spacy include the list below and the abbreviation spacy uses for that language. To choose a language simply type the two letter code following the angliscized language name in the list. So for Spanish it would be `'es'` (with the quotes) and for German `'de'` and so on.\n",
    "\n",
    "    - **English:** `'en'`\n",
    "    - **Chines:** `'zh'`\n",
    "    - **Spanish:** `'es'`\n",
    "    - **German:** `'de'`\n",
    "    - **French:** `'fr'`\n",
    "    - **Italian:** `'it'`\n",
    "    - **Portuguese:** `'pt'`\n",
    "    - **Japanese:** `'ja'`\n",
    "    - **Russian:** `'ru'`\n",
    "    - **Multi-Language:** `'xx'`\n",
    "    \n",
    "\n",
    "- **encoding/errors:** The variable `encoding` is where you determine what type of encoding to use (ascii, ISO-8850-1, utf-8, etc...). We have it set to utf-8 at the moment as we have found it is less likely to have any problems. However, errors do occur, but the encoding errors rarely impact our results and it causes the Python code to exit. So instead of dealing with unhelpful errors we ignore the ones dealing with encoding by assigning `'ignore'` to the `errors` variable. If you want to see any encoding errors then change `'ignore'` to `None` without the quotes.\n",
    "\n",
    "- **concat:** The HTRC Workset Toolkit gives the option of concatenating the HathiTrust volumes. If this option is chosen when downloading your corpus then all of the individual page files will be combined into one volume file. If you did not choose to concatenate your volumes, then there will be a directory for each volume and each directory contains a file for every page of that volume. Later on in the code there will be two options for reading in your corpus, one for if you concatenated the volumes, and one for if you did not. So the code know which one to choose we need to assign `True` to the `concat` variable if you did concatenate the volume pages or `False` if you did NOT concatenate the volume pages.\n",
    "\n",
    "- **n_comp:** The LSA algorithm used by the sklearn Python package has a required parameter called `n_components` for \"number of components\" (the point of it will be explained later). There is a part of the code further down that will help determine what the best number of components is for your corpus to produce the most accurate result. This will also make the process take longer as it is an added complicated step. If you want to perform this part of the code and try to determine the  best number of components, then assign `True` to the `n_comp` variable. If you do not wish to perform this added step then assign `False` to the `n_comp` variable. The default number of components will be the number of volumes in your corpus if you assign `False` to `n_comp`.\n",
    "\n",
    "- **stop_words =[]:** The `stop_words =[]` variable is simply an empty list. This is where the words from the nltk stopword list or your custom stopword list or both combined or neither (depending on what you decide) will reside later on. You do not need to do anything to this line of code.\n",
    "\n",
    "- **token_dict ={}:** The `token_dict = {}` variable is an empty dictionary. This is where your documents will reside later. The file name for the document will be the key and the content of the document will be the value. This will be explained in more detail later. For now, you do not need to do anything to this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stop = True\n",
    "custom_stop = False\n",
    "lem = True\n",
    "lower_case = True\n",
    "remove_digits = True\n",
    "language = 'english'\n",
    "lem_lang = \"en\"\n",
    "encoding = 'utf-8'\n",
    "errors = 'ignore'\n",
    "concat = False\n",
    "n_comp = True\n",
    "stop_words = []\n",
    "token_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "If you set `nltk_stop` equal to **True** above then this will add the NLTK stopwords list to the empty list named `stop_words`\n",
    "\n",
    "You should have already chosen your desired language above, but if you wish to add any words to the stopWords list then add the word(s) you want as a stop word in the `stop_words.extend(['words', 'you', 'want', 'to', 'add'])` part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nltk_stop is True:\n",
    "    # NLTK Stop words\n",
    "    stop_words = stopwords.words(language)\n",
    "\n",
    "    stop_words.extend(['would', 'said', 'says', 'also', '-PRON-', '-pron-'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add own stopword list\n",
    "\n",
    "Here is where your own stopwords list is added if you set `custom_stop` equal to **True** above. Here you will need to change the folder names and file name to match your folders and file. Remember to put each folder name in quotes and in the correct order always putting the file name including the file extension (.txt) last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if custom_stop is True:\n",
    "    stop_words_filepath = os.path.join(home_path, \"data\", \"my_stopwords.txt\")\n",
    "\n",
    "    with open(stop_words_filepath, \"r\",encoding = encoding, errors = errors) as f:\n",
    "        stop_words_list = [x.strip() for x in f.readlines()]\n",
    "\n",
    "    stop_words.extend(stop_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "We need to create a function in order to stem and tokenize our data. Any time you see `def` that means we are **DE**claring a **F**unction. The `def` is usually followed by the name of the function being created and then in parentheses are the parameters required by the function. After the parentheses is a colon, which closes the declaration, then a bunch of code below which is indented. The indented code is the program statement or statements to be executed. Once you have created your function all you need to do in order to run it is call the function by name and make sure you have included all the required parameters in the parentheses. This allows you to call the function without having to write out all the code in the function every time you wish to perform that task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization functions\n",
    "Here we have several \"if...else\" statements. First, if we set `lem` to **True** above we create two functions. One to ignore the \"\\n\" markers in our text and the next to tokenize and lemmatize our text. Second, within that first `if` statement, if `lem_lang` is set to a certain language abbreviation above, we want to use the lemmatization for that language. \n",
    "\n",
    "The `else` is if we set `lem` to **False** above then we create one function that tokenizes our text only.\n",
    "\n",
    "You should not need to make any changes to this block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lem is True:\n",
    "    if lem_lang == \"en\" or \"zh\":\n",
    "        nlp = spacy.load(lem_lang+\"_core_web_sm\", disable=[\"parser\",\"ner\"])\n",
    "    elif lem_lang == \"xx\":\n",
    "        nlp = spacy.load(lem_lang+\"_ent_wiki_sm\", disable=[\"parser\",\"ner\"])\n",
    "    else:\n",
    "        nlp = spacy.load(lem_lang+\"_core_news_sm\", disable = [\"parser\",\"ner\"])\n",
    "        \n",
    "    nlp.max_length=1500000\n",
    "    def token_filter(token):\n",
    "        return not (token.is_space)\n",
    "    \n",
    "    def tokenize(text):\n",
    "        for doc in nlp.pipe([text]):\n",
    "            tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
    "        return tokens\n",
    "\n",
    "else:\n",
    "    def tokenize(text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in documents\n",
    "Now we read in our documents and also perform some text cleaning. This code lower cases all the words as well as removes punctuation and digits, depending on what we set for the `lower_case` and `remove_digits` variables above. Then it adds the file names and cleaned content of each file to our previously empty `token_dict` dictionary above. You should not need to make any changes to this code.\n",
    "\n",
    "A dictionary is similar to a list except it has what are called 'keys' and 'values'. This basically allows us to label our data. In this case we will be making the file names of our documents the 'keys' and the content of the file the 'values' so that each document name correlates to the content of that document.\n",
    "\n",
    "The `if concat is True:` part says that if we assigned `True` to the `concat` variable above, then the code below the statement will be run, which reads in files as if each file was a separate volume (which it is if we concatenated the volume pages when we downloaded the volumes). If we did NOT assign `True` to `concat` (`else`), then the code below `else` will be run instead, which reads in the files for each directory and treats each directory as a single volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if concat is True:\n",
    "    for subdir, dirs, files in os.walk(data_home):\n",
    "        for file in files:\n",
    "            if file.startswith('.'):\n",
    "                    continue\n",
    "            if file.startswith('volume-rights.txt'):\n",
    "                    continue\n",
    "            if not file.endswith('.txt'):\n",
    "                    continue\n",
    "            \n",
    "            file_path = subdir + os.path.sep + file\n",
    "            with open(file_path, 'r', encoding = encoding, errors = errors) as text_file:\n",
    "                text = text_file.read()\n",
    "                if lower_case and remove_digits is True:\n",
    "                    lowers = text.lower()\n",
    "                    no_punctuation = lowers.translate(str.maketrans('','', string.punctuation))\n",
    "                    no_digits = no_punctuation.translate(str.maketrans('','', string.digits))\n",
    "                    token_dict[file] = no_digits\n",
    "                elif lower_case == True and remove_digits == False:\n",
    "                    lowers = text.lower()\n",
    "                    no_punctuation = lowers.translate(str.maketrans('','', string.punctuation))\n",
    "                    token_dict[file] = no_punctuation\n",
    "                elif lower_case == False and remove_digits == True:\n",
    "                    no_punctuation = text.translate(str.maketrans('','', string.punctuation))\n",
    "                    no_digits = no_punctuation.translate(str.maketrans('','', string.digits))\n",
    "                    token_dict[file] = no_digits\n",
    "                else:\n",
    "                    no_punctuation = text.translate(str.maketrans('','', string.punctuation))\n",
    "                    token_dict[file] = no_punctuation\n",
    "else:\n",
    "    \n",
    "    data = []\n",
    "    text = []\n",
    "    for folder in sorted(os.listdir(data_home)):\n",
    "        if not os.path.isdir(os.path.join(data_home, folder)):\n",
    "            continue\n",
    "        for file in sorted(os.listdir(os.path.join(data_home, folder))):\n",
    "            data.append(((data_home, folder,file)))\n",
    "    df = pd.DataFrame(data, columns = [\"Root\",\"Folder\", \"File\"])\n",
    "    df[\"Paths\"] = df[\"Root\"].astype(str) + \"/\" + df[\"Folder\"].astype(str) + \"/\" + df[\"File\"].astype(str)\n",
    "    for path in df[\"Paths\"]:\n",
    "        if not path.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open (path, \"r\", encoding = encoding, errors = errors) as f:\n",
    "            t = f.read().strip().split()\n",
    "            if lower_case and remove_digits is True:\n",
    "                lowers = ' '.join(t).lower()\n",
    "                no_punctuation = lowers.translate(str.maketrans('','', string.punctuation))\n",
    "                no_digits = no_punctuation.translate(str.maketrans('','', string.digits))\n",
    "                text.append(no_digits)\n",
    "            elif lower_case == True and remove_digits == False:\n",
    "                lowers = ' '.join(t).lower()\n",
    "                no_punctuation = lowers.translate(str.maketrans('','', string.punctuation))\n",
    "                text.append(no_punctuation)\n",
    "            elif lower_case == False and remove_digits == True:\n",
    "                no_punctuation = ' '.join(t).translate(str.maketrans('','', string.punctuation))\n",
    "                no_digits = no_punctuation.translate(str.maketrans('','', string.digits))\n",
    "                text.append(no_digits)\n",
    "            else:\n",
    "                no_punctuation = ' '.join(t).translate(str.maketrans('','', string.punctuation))\n",
    "                text.append(no_punctuation)\n",
    "                \n",
    "    \n",
    "    df[\"Text\"] = pd.Series(text)\n",
    "    df[\"Text\"] = [\"\".join(map(str, l)) for l in df[\"Text\"].astype(str)]\n",
    "    d = {'Text':'merge'}\n",
    "    df_text = df.groupby(['Folder'])[\"Text\"].apply(lambda x: ' '.join(x)).reset_index()\n",
    "    \n",
    "    token_dict = dict(zip(df_text[\"Folder\"], df_text[\"Text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check and see if our dictionary now has our data. We are asking to see the first 10 keys of our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['season_01', 'season_02', 'season_03', 'season_04', 'season_05', 'season_06', 'season_07']\n"
     ]
    }
   ],
   "source": [
    "print(list(token_dict.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf Vectorizer\n",
    "\n",
    "Here we weight the importance of each word in the document. This is done using Term Frequency-Inverse Document Frequency (Tfidf). This considers how important a word is based on the frequency in the whole corpus as well as in individual documents. This allows for words that might not have a high frequency in an entire collection, but do have a high frequency in one or two documents when compared to other words to still be given a higher level of importance throughout the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer = tokenize, stop_words = stop_words)\n",
    "dtm = vectorizer.fit_transform(token_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code outputs the first 20 words that make up the columns once we have broken our corpus down into a Tfidf matrix. To output all of the words remove the square brackets and their contents in the `print(vectGFN[:20])` line of code. To change the number of words printed change `20` in the same line to the number of words you wish to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x00\\x00\\x00\\x16', '\\x00\\x00\\x00\\x16b', '\\x00\\x00\\x00\\x16bb', '\\x00\\x00\\x00\\x16bbb', '\\x00\\x00\\x00\\x16bbbb', 'I', 'aa', 'aaaaall', 'aaaaard', 'aaagh', 'aaah', 'aah', 'aaron', 'ab', 'aback', 'aban', 'abandon', 'abash', 'abate', 'abatemarco']\n"
     ]
    }
   ],
   "source": [
    "# Get words that correspond to each column\n",
    "vectGFN = vectorizer.get_feature_names()\n",
    "print(vectGFN[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assigned `True` to the `n_comp` variable above, this cell will create a function that determines number of components that first gives us an explained variance ratio of our choice. The math is fairly complicated, but we are reducing our tfidf matrix to a more managable size, but we need to know how many components we can reduce our matrix to and still keep a certain percentage of our data. If we assigned `False` to the `n_comp` variable, this cell will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_comp is True:\n",
    "    tsvd = TruncatedSVD(n_components=dtm.shape[0])\n",
    "    tsvd.fit(dtm)\n",
    "    tsvd_var_ratios = tsvd.explained_variance_ratio_\n",
    "\n",
    "    def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "        total_variance = 0.0\n",
    "        n_components = 0\n",
    "        for explained_variance in var_ratio:\n",
    "            total_variance += explained_variance\n",
    "            n_components += 1\n",
    "            if total_variance >= goal_var:\n",
    "                break\n",
    "        return n_components\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assigned `True` to the `n_comp` variable we will now apply the function we created in the cell above. This will now incremently increase the number of components we will reduce our matrix to until we get a 95% or higher variance ratio, meaning we are still keeping 95% or more of our data. If we assigned `False` to `n_comp` then this cell will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "if n_comp is True:\n",
    "    nc = select_n_components(tsvd_var_ratios, 0.95)\n",
    "    print(nc)\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run SVD and Cosine Similarity\n",
    "\n",
    "Here we run our Tfidf matrix created above through Singular Value Decomposition and then calculate the Cosine Similarity of the documents to one another. \n",
    "\n",
    "Singular Value Decomposition condenses our Tfidf matrix down a bit to make it easier to process. Here we also set the number of dimensions (`n_components`) , how many times we iterate over the corpus (`n_iter`), and then set the seed (`random_state`) so that the results are reproducable since sklearn uses a bit of randomization in their algorithm. At the moment the `random_state` is set to 42 which sets the seed for the random number generator, but feel free to adjust the number to get a slightly different output. Just make sure you keep the seed the same once you find one you like for reproducibility. If you assigned `True` to `n_comp` then the `n_components` parameter will be set to whatever was determined to be the best option by the `select_n_components` function above. If you assigned `False` to the `n_comp` variable then the `n_components` will be the number of volumes in your corpus.\n",
    "\n",
    "Cosine similarity is where we measure how similar the documents are to one another. The result is a number between -1 and 1 with 1 being a perfect match (which we will get when the document is compared to itself) and -1 being completely different which we might get if we have a document of all numbers and one of all words with no numbers at all. Usually, even documents that are about unrelated topics share some common words and so are not completely dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_comp is True:\n",
    "    lsa = TruncatedSVD(n_components = nc, n_iter = 1000, random_state = 42)\n",
    "    dtm_lsa = lsa.fit_transform(dtm)\n",
    "    cosine_sim = cosine_similarity(dtm_lsa)\n",
    "else:\n",
    "    lsa = TruncatedSVD(n_components = dtm.shape[0], n_iter = 1000, random_state = 42)\n",
    "    dtm_lsa = lsa.fit_transform(dtm)\n",
    "    cosine_sim = cosine_similarity(dtm_lsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save as .csv\n",
    "\n",
    "Now we save the results as a .csv file. First we name the output .csv file so it matches our data. We do this in the first line of the cell.\n",
    "\n",
    "Then we create the dataframe and say we want the rows and columns to be labeled with the file names. Then we sort the columns in alphanumeric order by column header, then we sort the rows alphanumericaly by row label.\n",
    "\n",
    "Finally, we export the dataframe as a .csv file.\n",
    "\n",
    "You can manipulate the .csv file in excel or some other spreadsheet software, or you can use it in the \"doc_sim_lsa_heatmap\" notebook that accompanies this notebook. However, it is not recommended to use the accompanying heatmap notebook if your corpus exceeds 100 volumes as the heatmap becomes unwieldy and difficult to read and would, therefore, not be helpful in understanding your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name = \"doc_similarity_matrix_default.csv\"\n",
    "\n",
    "df = pd.DataFrame(cosine_sim, index = token_dict.keys(), columns=token_dict.keys())\n",
    "df_s = df[sorted(df)]\n",
    "sorted_df = df_s.sort_index(axis = 0)\n",
    "np.fill_diagonal(sorted_df.values, np.nan)\n",
    "sorted_df.to_csv(os.path.join(cleaned_data, csv_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was adapted from https://www.datascienceassn.org/sites/default/files/users/user1/lsa_presentation_final.pdf at Colorado University, Boulder. Accessed on 02/01/2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
